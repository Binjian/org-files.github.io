<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>大型语言模型与人工智能</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css" />
<style> .figure p {text-align: center;}</style>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">大型语言模型与人工智能</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd0a4462">概述</a></li>
<li><a href="#org32f4640">大型语言模型的工程实现</a></li>
<li><a href="#org96f8828">展望和挑战</a></li>
</ul>
</div>
</div>
<p>
:ID:       a52aa49d-d9d0-4b3f-ba2b-d5eced50e7c6
</p>

<div id="outline-container-orgddbe093" class="outline-3">
<h3 id="orgddbe093">签到码</h3>
<div class="outline-text-3" id="text-orgddbe093">

<div class="figure">
<p><img src="./img/llm_images/register.jpg" title="满意度调查码" width="600pix" />
</p>
</div>
</div>
</div>
<div id="outline-container-orgd0a4462" class="outline-2">
<h2 id="orgd0a4462">概述</h2>
<div class="outline-text-2" id="text-orgd0a4462">
</div>

<div id="outline-container-orgd21cf0a" class="outline-3">
<h3 id="orgd21cf0a">技术进步</h3>
<div class="outline-text-3" id="text-orgd21cf0a">
<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 1: </span>技术进步</label><pre class="src src-mermaid" id="org33442d6">flowchart LR
    id1(((农业革命)))--&gt;id2(((工业革命)))--&gt;id3(((电力革命)))--&gt;id4(((信息革命)))--&gt;id5(((大数据)))
    id4--&gt;id6(((人工智能)))
    id2--&gt;id7(((蒸汽机)))
</pre>
</div>

<div class="NOTES">
<p>

</p>

<p>
新技术的出现导致社会的进步，人工智能被誉为新时代的电力
电力有坏处：
</p>
<ul class="org-ul">
<li>触电危险</li>
<li>基础设施昂贵</li>
<li>消灭旧的行业,产生新的行业和职业</li>
</ul>


<p>
大数据：
<a href="https://motherduck.com/blog/big-data-is-dead/">Jordan Tigani (ex Google Enguineering lead of BigQuery)大数据已死</a>
2011, 2017~2019,大数据并没有成为瓶颈
</p>
<ul class="org-ul">
<li>到不了大数据级别 GB</li>
<li>存储和计算正在分离</li>
<li>没有新业务，数据是线性增长的</li>
<li>人们只关心最近的数据</li>
<li>真正有大数据的公司，几乎从不查询全部数据, 2017</li>
<li>单机的计算能力大增</li>
</ul>

</div>
</div>
</div>
<div id="outline-container-org7c7eb52" class="outline-3">
<h3 id="org7c7eb52">科学观念的更新</h3>
<div class="outline-text-3" id="text-org7c7eb52">
<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 2: </span>科学进展</label><pre class="src src-mermaid" id="org1b30d82">mindmap
  root((科学))
  :::urgent large
    )物理(
      相对论&lt;br/&gt;量子力学
	核聚变
    )人工智能(
      深度神经网络
	蛋白质折叠与功能
	大型语言模型
    )生物(
      生命起源&lt;br/&gt;线粒体
	 mRNA疫苗&lt;br/&gt;犯罪学,考古
</pre>
</div>


<div class="NOTES">
<p>
三个领域发生巨大持久和深刻的变化
</p>
<ul class="org-ul">
<li>理解原理越深刻，应用影响越大，&#x2013;&gt; 革命性的应用
<ul class="org-ul">
<li>物理学案例：核聚变，宇宙的起源，恒星的形成，\(E=MC^2\) ，取之不尽用之不竭的安全能源，50 年以后&#x2013;&gt;5 年以后</li>
<li>生物学案例：真核细胞生物的生化起源：光合作用，细胞呼吸作用，线粒体，外星生命研究</li>
</ul></li>
<li>发现问题是取得进展的研究方向。</li>
<li>深刻理解会改变观念！</li>
</ul>


<p>
<a href="https://www.gatesnotes.com/The-Age-of-AI-Has-Begun">比尔盖茨 AI的时代开始了</a>
</p>
<ul class="org-ul">
<li>GUI 之后的第二次革命性的技术展示 2022.中旬&#x2013;&gt; 9 月</li>
</ul>

</div>
</div>
<div id="outline-container-org772bf86" class="outline-4">
<h4 id="org772bf86">什么是 ChatGPT？</h4>
<div class="outline-text-4" id="text-org772bf86">
<p>
<b><u>Chat</u></b> <b><u>G</u></b>enerative <b><u>P</u></b>retrained <b><u>T</u></b>ransformer
</p>
<ul class="org-ul">
<li>本质：智能转化为计算
<ul class="org-ul">
<li>计算的基本对象：内嵌空间 （ <b>embedding</b> ）</li>
<li>机器学习方法</li>
</ul></li>
<li>特点
<ul class="org-ul">
<li>大规模</li>
<li>单一的方法（深度学习 Transformer 架构）</li>
<li>多语言模式</li>
<li>强人工智能，AGI（？）</li>
</ul></li>
<li>开源开放
<ul class="org-ul">
<li>知道如何工程实现，根本原因不清楚</li>
<li>普遍适用其他复杂现象：图像，控制，可迁移</li>
<li>机缘巧合</li>
</ul></li>
</ul>
<div class="NOTES">
<ul class="org-ul">
<li>语料，训练样本
<ul class="org-ul">
<li>until 2003 5 EB ExaByte, 2013 5EB/2 days (1EB = \(10^9\) GB, 1 Zettabye = \(10^{12}\) GB, billions and billions Carl Sagan)</li>
<li>模型， 计算量</li>
<li>训练不充分</li>
<li>规模化的必要的，但很可能不是充分的</li>
</ul></li>
<li>单一的方法（人工智能，机器学习，深度学习，大型神经网络模型， 计算模型）：1990s 就有，计算量，互联网的兴起
<ul class="org-ul">
<li>工程实现原理完全清楚，结果需要解释和分析，有争议</li>
<li>赌注，stake，自信，勇气，信仰</li>
</ul></li>
<li>偶然 Serendipity
<ul class="org-ul">
<li>硬件彩票 GPU
<ul class="org-ul">
<li>1990 64 个节点的计算机网络，jeff dean, Yoshua Bengio</li>
</ul></li>
<li>专家：李飞飞, Hinton, Bengio, LeCunn</li>
<li>偶然中的必然：生命的生化起源，真核生物的起源，语言的起源（20 万年前）; 演化推动指数级增长</li>
</ul></li>
<li>工程实现理解
<ul class="org-ul">
<li>可视化，动画的方式（Jay Alammar, Lilian Weng, Christopher Potts）</li>
<li>剥洋葱的方式，一层层往里看</li>
</ul></li>
</ul>

</div>
</div>
</div>


<div id="outline-container-org42e291e" class="outline-4">
<h4 id="org42e291e">Ilya Sutskever NIPS 2015</h4>
<div class="outline-text-4" id="text-org42e291e">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/sutskever_nips2015.png" title="Sutskever 2015" width="400pix" align="center" />
</p>
</div>
<blockquote>
<ul class="org-ul">
<li>如果数据集够大</li>
<li>并且训练一个很大的神经网络</li>
<li>你肯定能成功!</li>
</ul>
</blockquote>
<div class="NOTES">
<p>
RNN 模型，谷歌大脑
<a href="https://www.youtube.com/watch?v=-uyXE7dY5H0">https://www.youtube.com/watch?v=-uyXE7dY5H0</a>
</p>

</div>
</div>
</div>

<div id="outline-container-orgc3353d3" class="outline-4">
<h4 id="orgc3353d3">大型语言模型</h4>
<div class="outline-text-4" id="text-orgc3353d3">
</div>
<ul class="org-ul">
<li><a id="org42edb22"></a>GPT 系列<br />
<ul class="org-ul">
<li><a id="org59c9675"></a>GPT2 (1.5B), GPT3 (175B), InstructGPT(Alignment, RLHF)， ChatGPT(数据收集差异), GPT4(?)<br />
<div class="outline-text-6" id="text-org59c9675">
<p>
👉 NanoGPT (Andrej Karpathy)
</p>
<ul class="org-ul">
<li><a href="https://www.salesforce.com/news/wp-content/uploads/sites/3/2023/03/Slack_ChatGPT_Blue.gif">ChatGPT for Slack</a></li>
</ul>


<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/Slack_ChatGPT_Blue.gif" title="神经网络做为大型语言模型" width="600px" align="center" />
</p>
</div>
</div>
</li>
</ul>
</li>
</ul>
</div>

<div id="outline-container-orgf205696" class="outline-4">
<h4 id="orgf205696">大型语言模型及训练计算量</h4>
<div class="outline-text-4" id="text-orgf205696">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/Ai-training-computation.png" title="隐空间聚类分布" width="500pix" align="center" />
</p>
</div>
<ul class="org-ul">
<li>Google: LaMDA(137B),PaLM(540B, Minerva,PaLM-E),BERT(0.34B)</li>
<li>Meta: Galactica,OPT(175B),LLaMA（65B）</li>
<li>MS&amp;NV: Megatron(530B)</li>
<li>DM: <b>Chinchilla</b> (70B)</li>
<li>HF🤗:Bloom(175B)</li>
<li>EleutherAI: GPT-NEO(2.7B),-J(6B),-NeoX(20B)</li>
<li>DALL-E, Imagen, Flamingo, Parti, SD</li>
</ul>
<div class="NOTES">
<p>
模型大小：神经网络参数个数（推理），训练消耗的计算量
</p>

<p>
计算问题！
</p>

<p>
kiloFlops 10^3, metaFlops 10^6, giga- 10^9（十亿）, tera- 10^12(万亿), peta- 10^15（千万亿）, exa- 10^18（百万万亿，百亿亿, zetta- 10^21（万万万亿）, yotta- 10^24, ronna- 10^27, quetta-10^30
</p>

<p>
Palm Pathway Languane model, -e embodied, open API （3.14）
Chinchilla 模型和意义： 所有的模型：训练不足，模型太大, undertrained
Amazon: AlexaTM(20B)
</p>

</div>
</div>
</div>

<div id="outline-container-org0818e2f" class="outline-4">
<h4 id="org0818e2f">锂电池能量密度提升</h4>
<div class="outline-text-4" id="text-org0818e2f">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/FOTW_1234.png" title="锂电池能量密度的增长" width="800px" align="center" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgd660fff" class="outline-4">
<h4 id="orgd660fff">大型语言模型的能力改善</h4>
<div class="outline-text-4" id="text-orgd660fff">

<div class="figure">
<p><img src="./img/llm_images/llm-progress.jpg" title="Emergence Behavior" width="500px" />
</p>
</div>
<div class="NOTES">
<p>
2012 AlexNet(PC)
2017 Transformer(Attention)
爆炸性发展
</p>

</div>
</div>
</div>

<div id="outline-container-org7058907" class="outline-4">
<h4 id="org7058907">社会影响</h4>
<div class="outline-text-4" id="text-org7058907">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/ai_investment.png" alt="人工智能的投入" title="人工智能的投入" width="600px" align="right" />
</p>
</div>
<ul class="org-ul">
<li>微软入股 OpenAI 100 亿美元，持股增至 49%，</li>
<li>人工智能军备竞赛：微软(Sydney)，谷歌(LLaMDA, Bard)，Meta(Galactica, LlaMa), GPT4 发布</li>
<li>智能(Intelligence)，能动性(Agency)，知觉（Sentience)，意识(Conciousness)，意志（Free Will)&#x2026;
<ul class="org-ul">
<li><a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">Washington Post LaMDA Report</a></li>
</ul></li>
</ul>
<div class="NOTES">
<ul class="org-ul">
<li>复旦邱锡鹏 MOSS</li>
<li>智能的物质基础?</li>
<li>智能是人性根本性的一部分？（从人性中分离？）</li>
<li>黄易山 Yishan Wong,前 reddit CEO(2012-2014) 预言 2023 年底会发生某个奇点事件！</li>
</ul>

</div>
</div>
</div>
<div id="outline-container-orgcbd040c" class="outline-4">
<h4 id="orgcbd040c"><a href="https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html">ChatGPT的虚假承诺</a></h4>
<div class="outline-text-4" id="text-orgcbd040c">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/Noam_Chomsky_portrait_2017_retouched.png" title="诺姆⋅乔姆斯基" width="400pix" align="center" />
</p>
</div>
<blockquote>
<p>
所谓人工智能革命性的进展令人既担忧又乐观。
乐观是因为智能可以用于解决问题，担忧是因为当今最流行的人工智能方法，也就是机器学习，它的语言和知识的概念从根本上是有缺陷的。
</p>
</blockquote>
<div class="NOTES">
<p>
这种机器学习方法把这些内含缺陷的概念整合到我们的技术和产品中， 从而贬低了我们的科学和道德伦理。
The human mind is not, like ChatGPT and its ilk, a lumbering statistical engine for pattern matching, gorging on hundreds of terabytes of data and extrapolating the most likely conversational response or most probable answer to a scientific question. On the contrary, the human mind is a surprisingly efficient and even elegant system that operates with small amounts of information; it seeks not to infer brute correlations among data points but to create explanations.
</p>

<p>
批评：Oxford Summerfield Lab:"Like others, Chomsky pits “pattern matching” vs. “understanding”. this is a sort of neo-dualism: it diminishes computation by asserting that it lacks some intangible quality (as we might diminish other minds by assuming they lacks some intangible quality (as we might diminish other minds by assuming they lack subjectivity)
</p>

<p>
从佛教角度，二元论夸大“我相”，执迷
</p>

</div>
</div>
</div>

<div id="outline-container-orgb25c252" class="outline-4">
<h4 id="orgb25c252"><a href="https://venturebeat.com/ai/as-gpt-4-chatter-resumes-yoshua-bengio-says-chatgpt-is-a-wake-up-call/">Yoshua Bengio</a></h4>
<div class="outline-text-4" id="text-orgb25c252">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/Yoshua_Bengio_2019_cropped.jpg" title="诺姆⋅乔姆斯基" width="400pix" align="center" />
</p>
</div>
<blockquote>
<p>
ChatGPT 令人印象深刻，但在科学上只是微小的一步，最多称得上是工程上的进展。它的主要意义在于唤醒公众对人工智能意义的认识。
</p>
</blockquote>
<div class="NOTES">
<ul class="org-ul">
<li>1990s：1991 "ANN and their application to sequence recognition"</li>
<li>2000s：2003 "A Neural Probabilistic Language Model" 理解大型语言模型的基础！</li>
<li>2010s：2014 "Neural Machine Translation by Jointly Learning to Align and translate"</li>
<li>2018 图灵奖</li>
<li>2010 年以前，相信这种方法能成功的屈指可数！</li>
</ul>


<p>
2000s: embedding 代替 n-gram n 元语法，Markov 链
</p>
<ol class="org-ol">
<li>数学模型</li>
<li>优化方法（表达和实现方式）</li>
</ol>

</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org32f4640" class="outline-2">
<h2 id="org32f4640">大型语言模型的工程实现</h2>
<div class="outline-text-2" id="text-org32f4640">
</div>
<div id="outline-container-org2439136" class="outline-3">
<h3 id="org2439136">用例</h3>
<div class="outline-text-3" id="text-org2439136">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/nn.png" title="神经网络做为大型语言模型" width="500px" align="center" />
</p>
</div>
<div class="NOTES">
<ul class="org-ul">
<li>熟悉的方案：图像，语音，控制，下棋，自然语言</li>
<li>无论输入源连续离散都是一种处理方式：自然语言本质上是离散的，图像，语音和控制策略本质上是连续的。（？）</li>
<li>多层感知机是最广义的神经网络，包含所有其他的网络类型。断开某些连接即可，比如卷积网</li>
<li>信号数学模型+信号的处理模型（网络）</li>
</ul>

</div>
</div>
</div>

<div id="outline-container-org993bd8d" class="outline-3">
<h3 id="org993bd8d">语言编码模型：语素和 n-元语法(n-gram)</h3>
<div class="outline-text-3" id="text-org993bd8d">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/ngram-example.png" title="n 元语法（n-gram）" width="500px" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/ngram-model.gif" title="n 元语法（n-gram）" width="500px" align="center" />
</p>
</div>
<div class="NOTES">
<ul class="org-ul">
<li>语素的设计参数选择：字母，音素，音节，单词，</li>
<li>统计方法优化选择（无监督学习，Byte-Pair-Encoding）：google sentencepiece; openai tiktoken</li>
<li>马尔可夫链：复杂度随维度的诅咒</li>
</ul>

</div>
</div>
</div>

<div id="outline-container-org9b4a860" class="outline-3">
<h3 id="org9b4a860">GPT 中的计算对象：内嵌(embedding)</h3>
<div class="outline-text-3" id="text-org9b4a860">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/word2vec.png" title="内嵌" width="500px" align="center" />
</p>
</div>
<ol class="org-ol">
<li>内嵌（单词/语素的编码）
<ul class="org-ul">
<li>独立语义，在句子/文本的不同位置重复出现，可以复用的变量</li>
<li>对应于感质（Quolia）：概念（颜色）在意识中的聚类，语言只是一种接口</li>
</ul></li>
<li>内嵌的相互关系通过计算确认</li>
<li>内嵌通过训练样本学习，收集由句法确定的语义</li>
<li><a href="https://projector.tensorflow.org/">预训练内嵌空间（tensorflow）</a></li>
</ol>
<div class="NOTES">
<ul class="org-ul">
<li>内嵌空间（embedding）：概念空间 , （统计方法确定的）</li>
<li>内嵌不是语素，是对语素进行编码得到的，需要端到端训练,token 令牌，约等于单词 100 token 约等于 75 个单词</li>
<li>内嵌对应人类语言中的概念（quolia 感质）：离散的，吸收的。（Yoshua Bengio: quolia,离散，概念空间的引力中心）</li>
<li>线性组合，简单的矩阵运算</li>
<li>网络的权重系数：矩阵运算的系数，对应这些概念之间的联系</li>
<li>神经网络：分布式表达模型</li>
</ul>

</div>
</div>
</div>
<div id="outline-container-org1cae5b5" class="outline-3">
<h3 id="org1cae5b5">内嵌的运算（embedding）</h3>
<div class="outline-text-3" id="text-org1cae5b5">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/king-colored-embedding.png" alt="King, Man, Woman" width="800pix" title="内嵌向量" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/king-man-woman-embedding.png" title="隐空间聚类分布" width="800pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/queen-woman-girl-embeddings.png" title="隐空间聚类分布" width="800pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/king-analogy-viz.png" title="隐空间聚类分布" width="800pix" align="center" />
</p>
</div>
<div class="NOTES">
<p>
数据（单词）本身是有结构的,相互关系，出现的频率，相似性，交换性，位置（语法，句法)的含义。
由神经网络分布式地表达：概念之间的关系，运算（神经脉冲的传导）
万物都有一种模式，它是我们宇宙的一部分。 它具有对称、优雅和魅力——您总能在真正的艺术家描绘的东西中发现这些品质。 你可以在季节的交替中，在沙子沿着山脊的轨迹中，在杂酚油灌木丛的枝条丛中或它的叶子的图案中找到它。
我们试图在我们的生活和社会中复制这些模式，寻找节奏、舞蹈和令人舒适的形式。 然而，在寻找终极完美的过程中可能会看到危险。 很明显，最终模式包含它自己的固定性。 在这样的完美中，万物都走向死亡。
“There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote bush or the pattern of its leaves.
We try to copy these patterns in our lives and our society, seeking the rhythms, the dances, the forms that comfort. Yet, it is possible to see peril in the finding of ultimate perfection. It is clear that the ultimate pattern contains it own fixity. In such perfection, all things move toward death.” ~ Dune (1965)
</p>

</div>
</div>
</div>
<div id="outline-container-orgc81089a" class="outline-3">
<h3 id="orgc81089a">图像中的内嵌</h3>
<div class="outline-text-3" id="text-orgc81089a">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/image_embedding.png" title="图像内嵌" width="800px" align="center" />
</p>
</div>
<ol class="org-ol">
<li>图像内嵌编码和解码，通过 DCGAN 训练得到</li>
<li>内嵌参数的插值：图像的连续变化（男&#x2013;&gt;女）</li>
<li>内嵌的向量运算：图像的修改</li>
</ol>
<div class="NOTES">
<ul class="org-ul">
<li>Alec Radford now at OpenAI, credit for OpenAI LLM</li>
</ul>

</div>
</div>
</div>

<div id="outline-container-org8e20f16" class="outline-3">
<h3 id="org8e20f16"><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">GPT3 训练</a></h3>
<div class="outline-text-3" id="text-org8e20f16">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/01-gpt3-language-model-overview.gif" alt="Overview" title="overview" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/02-gpt3-training-language-model.gif" alt="training" title="Training" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/gpt3-training-examples-sliding-window.png" alt="training samples" title="training samples" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/03-gpt3-training-step-back-prop.gif" title="隐空间聚类分布" width="500pix" align="center" />
</p>
</div>
<div class="NOTES">
<ol class="org-ol">
<li>预训练模型生成文本</li>
<li>单一大模型训练：355GPU years $4.6M， 300 B (token, 单词，词干/词根）</li>
<li>训练样本生成</li>
<li>训练：预测下个单词,根据目标修正参数（175 B)</li>
<li>数据
<ul class="org-ul">
<li>网络文本</li>
<li>代码</li>
<li>英语</li>
</ul></li>
<li>基于上下文理解的训练</li>
</ol>

</div>
</div>
</div>

<div id="outline-container-org7b48235" class="outline-3">
<h3 id="org7b48235"><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">GPT3 推理</a></h3>
<div class="outline-text-3" id="text-org7b48235">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/04-gpt3-generate-tokens-output.gif" alt="Overview" title="overview" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/gpt3-parameters-weights.png" alt="training" title="Training" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/05-gpt3-generate-output-context-window.gif" alt="training samples" title="training samples" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/06-gpt3-embedding.gif" title="隐空间聚类分布" width="500pix" align="center" />
</p>
</div>
<div class="NOTES">
<ol class="org-ol">
<li>生成模型(Generative):推理一次生成一个单词;序列，自回归模型;对概率分布的采样,是随机的。多模态的根本原因。</li>
<li>无监督学习预训练生成有用的参数</li>
<li>上下文最大长度：2048 (2k);GPT-4 0.03+0.06/1k@8k, 0.06+0.12/1k@32k; ColT5 64K，自回归模型</li>
<li>基本步骤：1.单词转换成内嵌（编码），2.预测，3.内嵌还原成单词（解码）：内嵌的编码是端到端训练得到的。</li>
</ol>

</div>
</div>
</div>
<div id="outline-container-org181e688" class="outline-3">
<h3 id="org181e688"><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">GPT3 与 Transformer</a></h3>
<div class="outline-text-3" id="text-org181e688">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/07-gpt3-processing-transformer-blocks.gif" alt="Overview" title="overview" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/08-gpt3-tokens-transformer-blocks.gif" alt="training" title="Training" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/09-gpt3-generating-react-code-example.gif" alt="training samples" title="training samples" width="500pix" align="center" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/10-gpt3-fine-tuning.gif" title="隐空间聚类分布" width="500pix" align="center" />
</p>
</div>
<div class="NOTES">
<ol class="org-ol">
<li>96 个 transformer 解码层， 每个解码层参数~1.8B</li>
<li>解码过程</li>
<li>App React 代码生成</li>
<li>迁移学习（特殊任务的细调）： InstructGPT, ChatGPT</li>
</ol>
<p>
<a href="https://twitter.com/i/status/1284421499915403264">https://twitter.com/i/status/1284421499915403264</a>
</p>

</div>
</div>
</div>
<div id="outline-container-org7f42604" class="outline-3">
<h3 id="org7f42604">ChatGPT</h3>
<div class="outline-text-3" id="text-org7f42604">

<div class="figure">
<p><object type="image/svg+xml" data="file:///home/runner/.org.d/roam/img/llm_images/ChatGPT_Diagram.svg" class="org-svg" title="隐空间聚类分布" width="800pix" align="center">
Sorry, your browser does not support SVG.</object>
</p>
</div>
<ul class="org-ul">
<li>GPT3.5: codex</li>
<li>监督学习，细调</li>
<li>强化学习(PPO)构造奖励函数</li>
<li>应用强化学习训练改进模型</li>
</ul>
<div class="NOTES">
<ul class="org-ul">
<li>代码是高质量的语言</li>
<li>英语是一种严格的形式化语言（蒙塔尤）</li>
<li>汉语：图形化文字，语音上多样性不够，多音字，同音字，严重依赖上下文。图像性的优势，语音上的缺陷，语义上表达上有一定的模糊性，似是而非。谐音。</li>
<li>汉语样本训练（为辅）</li>
<li>乔姆斯基：普遍语法论,能学会外语，翻译的根本。</li>
<li>Meta，翻译对齐两个内嵌空间的映射关系。</li>

<li>为何强化学习？：解决长效奖励问题。</li>
</ul>

</div>
</div>
<div id="outline-container-org8079fe8" class="outline-4">
<h4 id="org8079fe8">涌现行为（<a href="https://www.jasonwei.net/blog/emergence">Emergence Behavior</a>)</h4>
<div class="outline-text-4" id="text-org8079fe8">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/emergence.gif" title="Emergence Ablities on Benchmarks" width="600px" />
</p>
</div>

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/emergence_behavior.jpeg" title="Emergence Behavior" width="600px" />
</p>
</div>
<div class="NOTES">
<p>
温度作为物理现象：液态水，蒸汽，水分子到达一定量级才会出现
</p>

</div>
</div>
</div>
</div>

<div id="outline-container-orgd9daf27" class="outline-3">
<h3 id="orgd9daf27">应用和部署</h3>
<div class="outline-text-3" id="text-orgd9daf27">
<ul class="org-ul">
<li>提示工程(Prompt Engineering)</li>
<li>LLaMA 复刻 GPT (斯坦福<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a> 7B, 100$）
<ul class="org-ul">
<li>通过 API 比对训练➡商业模式？</li>
</ul></li>
<li>LLaMA (7B) 树莓派移植（4GB, 10sec/token）</li>
</ul>

<div class="NOTES">
<p>
提示工程：
人补充长逻辑依赖问题，弥补神经网络长序列理解问题（2k 序列长度），
与人工智能对话：
</p>
<ul class="org-ul">
<li>准确描述输入需求</li>
<li>匹配模型多模态</li>
</ul>

</div>
</div>
</div>
<div id="outline-container-org6d1fb24" class="outline-3">
<h3 id="org6d1fb24">变形金刚（Transformer）</h3>
<div class="outline-text-3" id="text-org6d1fb24">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/transformer.png" alt="Transformer and attention" title="Transformer and attention" width="800px" align="right" />
</p>
</div>
<ul class="org-ul">
<li>结构: 归纳偏差少，通用性好
<ul class="org-ul">
<li>注意力（内注意力（self attention)，交叉注意力， 多头内注意力</li>
<li>MLP,多层感知机</li>
<li>残差结构</li>
</ul></li>
<li>需要大量的训练样本</li>
<li>网络尺度和数据集</li>
</ul>
<div class="NOTES">
<p>
位置编码
层归一化
GPT3
软注意力，硬注意力
卷积网的权重系数用另一个网络生成：二阶网络
</p>

</div>
</div>
</div>

<div id="outline-container-org3a48b8a" class="outline-3">
<h3 id="org3a48b8a">争议</h3>
<div class="outline-text-3" id="text-org3a48b8a">
</div>
<div id="outline-container-org8356b82" class="outline-4">
<h4 id="org8356b82"><a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web">ChatGPT 是现实的模糊版本</a></h4>
<div class="outline-text-4" id="text-org8356b82">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/Chiang.gif" title="blurry web" width="800pix" align="center" />
</p>
</div>
<p>
ChatGPT 是现实的模糊复印
</p>
<ul class="org-ul">
<li>类似 jpeg 图片对原始图片的有损压缩</li>
<li>现实的描述不精确，造成语料和信息的失真和模糊</li>
<li>模型幻觉问题（hallucination），造成语料和信息的污染</li>
<li>有损压缩显得更智能</li>
</ul>
<div class="NOTES">
<p>
生成模型的高效迅速放大污染问题
Markus Hutter Prize 2006 智能是一种压缩，无损压缩: 1GB wiki &#x2013;&gt; 115MB
</p>

</div>
</div>
</div>
<div id="outline-container-org4eed056" class="outline-4">
<h4 id="org4eed056"><a href="https://www.fast.ai/posts/2023-03-20-wittgenstein.html">GPT4 和语言的未知领域</a></h4>
<div class="outline-text-4" id="text-org4eed056">
<p>
“它们(LLM)还可能带来新的伦理、社会和文化挑战，需要认真反思和监管。 我们如何使用这项技术将取决于我们如何认识到它对我们自己和他人的影响。
</p>

<p>
该技术是“人工智能”的一种形式。 “智能”一词源自 inter-（“之间”）和 legere（“选择、挑选、阅读”）。 那么，智能就是能够在事物之间做出选择，挑选出重要的东西，阅读所写的东西。 智力不仅仅是数量或质量； 它是一种活动、一种过程、一种实践。 这是我们用思想和语言做的事情。
</p>

<p>
但是当我们让 GPT4 为我们做这件事时，我们不是在放弃我们的智能吗？ 难道我们没有放弃选择、挑选、阅读的能力吗？ 我们不是变成了语言的被动消费者而不是主动的生产者吗？”
</p>


<div class="NOTES">
<p>
Jeremy Howard 2023.02.23
<a href="https://www.fast.ai/posts/2023-03-20-wittgenstein.html">GPT 4 and the Uncharted Territories of Language</a>
</p>

<p>
“The limits of my language mean the limits of my world.” — Ludwig Wittgenstein
</p>

<p>
They could also create new ethical, social, and cultural challenges that require careful reflection and regulation. How we use this technology will depend on how we recognize its implications for ourselves and others.
</p>

<p>
This technology is a form of “Artificial Intelligence”. The word “intelligence” derives from inter- (“between”) and legere (“to choose, pick out, read”). To be intelligent, then, is to be able to choose between things, to pick out what matters, to read what is written. Intelligence is not just a quantity or a quality; it is an activity, a process, a practice. It is something that we do with our minds and our words.
</p>

<p>
But when we let GPT 4 do this for us, are we not abdicating our intelligence? Are we not letting go of our ability to choose, to pick out, to read? Are we not becoming passive consumers of language instead of active producers?
</p>

</div>
</div>
</div>

<div id="outline-container-orga02bed2" class="outline-4">
<h4 id="orga02bed2"><a href="https://sohl-dickstein.github.io/2023/03/09/coherence.html">智能与一致性问题</a></h4>
<div class="outline-text-4" id="text-orga02bed2">

<div class="figure">
<p><img src="file:///home/runner/.org.d/roam/img/llm_images/int_coh_cartoon_1.png" title="智能与条理性（coherence)" />
</p>
</div>
</div>
</div>
<div id="outline-container-org4bb6759" class="outline-4">
<h4 id="org4bb6759">越高级的智能越混乱</h4>
<div class="outline-text-4" id="text-org4bb6759">

<div class="figure">
<p><img src="./img/llm_images/int_coh_life.png" width="800pix" title="生物智能条理性" />
</p>
</div>

<div class="figure">
<p><img src="./img/llm_images/int_coh_organization.png" width="800pix" title="社会组织的条理性" />
</p>
</div>
</div>
</div>

<div id="outline-container-org1ce1ebb" class="outline-4">
<h4 id="org1ce1ebb">神经网络的条理性</h4>
<div class="outline-text-4" id="text-org1ce1ebb">

<div class="figure">
<p><img src="./img/llm_images/int_coh_machines.png" title="神经网络的条理性" width="800pix" />
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org96f8828" class="outline-2">
<h2 id="org96f8828">展望和挑战</h2>
<div class="outline-text-2" id="text-org96f8828">
<ul class="org-ul">
<li>效率，开放，出处，有效性，合成
<ul class="org-ul">
<li>基于检索（搜索）的自然语言处理</li>
</ul></li>
<li>大型语言模型的“最后一公里”</li>
<li>网络结构理解
<ul class="org-ul">
<li>维护，高效更新</li>
</ul></li>
<li>缺点
<ul class="org-ul">
<li>长段落</li>
<li>长逻辑推理（chain-of-thought reasoning）
👉 强化学习？</li>
<li>自然语料样本空间的污染</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orge83fcad" class="outline-3">
<h3 id="orge83fcad">满意度调查码</h3>
<div class="outline-text-3" id="text-orge83fcad">

<div class="figure">
<p><img src="./img/llm_images/feedback.jpg" title="满意度调查码" width="600pix" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2023-03-13 Mon 00:00</p>
<p class="creator">忻斌健</p>
</div>
</body>
</html>
