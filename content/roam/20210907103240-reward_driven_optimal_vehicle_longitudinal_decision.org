:PROPERTIES:
:ID:       38694c8a-c239-4a09-8a9a-5deeaffa0212
:END:
#+title: reward-driven optimal vehicle longitudinal decision


* comparison
** 基于深度强化学习的混动列车的能量管理方法及系统 （2020）
*** 区别
**** 混动
**** 列车
**** 仿真
***** 需要建立动力仿真模型
***** 我们不需要建立模型，是无模型方法，本质上从大数据中提取更新复杂模型
***** 基于仿真
**** 离线训练
***** 非闭环控制，非动态控制
****** 离线用仿真模型训练，得到应用静态策略
***** 训练使用仿真的数据，而不是真实车辆行驶数据，没有使用车辆真实行驶数据进行训练，
***** 能量管理策略在真实环境中运行的性能和效果主要依赖于仿真模型的精度，而不是依赖于大数据，
***** 本质上不是基于大数据的方法
*** 相似
**** 源数据相当于观测量，有车速，加速度，电池电量，没有人员操作，没有电压电流值
** 基于 QoE 的个性化自动驾驶参数优化设定方法（2014）
*** 不同
**** 场景离散划分，需要场景识别模块，且场景参数化是离散分段的，而不是连续的。
***** 我们不对场景进行参数化描述和存取
**** 没有使用深度学习和强化学习，是典型的老方法，无法规模化。
***** 参数化后使用 K-Means 等传统聚类方法进行参数匹配，需要人为设定一些关键参数，比如场景聚类个数，我们不需要。
***** 最佳参数的选择因为场景是离散化的，实际上并不是最佳的。
***** 随着数据的增加，场景复杂度增加，原有的关键参数必然会失效，就导致性能下降。
*** 相似
**** 云端：数据存在云端，数据以静态方式按车辆型号和离散的场景分类存取
**** 最佳参数选择是静态匹配方式
**** 云端数据静态存储管理，无有效处理大数据的方法，
